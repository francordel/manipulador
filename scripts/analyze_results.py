#!/usr/bin/env python3
"""
Results analysis script for Manipulador evaluations.

This script analyzes evaluation results from agents and ground truth baselines,
generates comparative statistics, and creates visualizations and LaTeX tables.
"""

import sys
import os
import json
import argparse
import csv
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import statistics
from datetime import datetime

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

class ResultsAnalyzer:
    """Analyzer for Manipulador evaluation results."""
    
    def __init__(self, verbose: bool = True):
        """Initialize results analyzer."""
        self.verbose = verbose
        self.agent_results = {}
        self.ground_truth_results = {}
        
    def load_agent_results(self, results_dirs: List[str]) -> Dict[str, Any]:
        """Load agent evaluation results from multiple directories."""
        all_results = {}
        
        for results_dir in results_dirs:
            results_path = Path(results_dir)
            if not results_path.exists():
                if self.verbose:
                    print(f"‚ö†Ô∏è  Results directory not found: {results_dir}")
                continue
            
            # Look for agent result files
            for result_file in results_path.glob("*_evaluation_results.json"):
                agent_type = result_file.stem.replace('_evaluation_results', '')
                
                with open(result_file, 'r') as f:
                    data = json.load(f)
                
                all_results[agent_type] = {
                    'file_path': str(result_file),
                    'data': data
                }
                
                if self.verbose:
                    print(f"‚úÖ Loaded {agent_type} results: {len(data.get('detailed_results', []))} evaluations")
        
        self.agent_results = all_results
        return all_results
    
    def load_ground_truth(self, ground_truth_dir: str) -> Dict[str, Any]:
        """Load ground truth baseline results."""
        gt_path = Path(ground_truth_dir)
        if not gt_path.exists():
            if self.verbose:
                print(f"‚ö†Ô∏è  Ground truth directory not found: {ground_truth_dir}")
            return {}
        
        # Look for complete ground truth file
        complete_file = gt_path / "complete_ground_truth.json"
        if complete_file.exists():
            with open(complete_file, 'r') as f:
                data = json.load(f)
            
            self.ground_truth_results = data
            
            if self.verbose:
                models = list(data.get('results_by_model', {}).keys())
                print(f"‚úÖ Loaded ground truth for {len(models)} models: {', '.join(models)}")
            
            return data
        
        # Look for individual model files
        ground_truth = {'results_by_model': {}}
        for model_dir in gt_path.glob("*/"):
            if model_dir.is_dir():
                gt_file = model_dir / f"{model_dir.name}_ground_truth.json"
                if gt_file.exists():
                    with open(gt_file, 'r') as f:
                        model_data = json.load(f)
                    ground_truth['results_by_model'][model_dir.name] = model_data
        
        self.ground_truth_results = ground_truth
        return ground_truth
    
    def compare_agent_performance(self) -> Dict[str, Any]:
        """Compare performance across different agents."""
        if not self.agent_results:
            return {}
        
        comparison = {
            'agents_compared': list(self.agent_results.keys()),
            'performance_comparison': {},
            'method_distribution_comparison': {},
            'statistical_comparison': {}
        }
        
        for agent_name, agent_data in self.agent_results.items():
            results = agent_data['data'].get('detailed_results', [])
            metadata = agent_data['data'].get('evaluation_metadata', {})
            summary = agent_data['data'].get('performance_summary', {})
            
            # Basic performance metrics
            total_evals = len(results)
            successful_evals = sum(1 for r in results if r.get('success', False))
            success_rate = successful_evals / total_evals * 100 if total_evals > 0 else 0
            
            # Method distribution
            method_counts = {}
            for result in results:
                method = result.get('selected_method', 'UNKNOWN')
                method_counts[method] = method_counts.get(method, 0) + 1
            
            method_distribution = {
                method: count / total_evals * 100 
                for method, count in method_counts.items()
            } if total_evals > 0 else {}
            
            # Execution time statistics
            execution_times = [r.get('execution_time', 0) for r in results if 'execution_time' in r]
            
            comparison['performance_comparison'][agent_name] = {
                'total_evaluations': total_evals,
                'successful_evaluations': successful_evals,
                'success_rate': success_rate,
                'total_runtime': metadata.get('total_runtime', 0),
                'avg_execution_time': statistics.mean(execution_times) if execution_times else 0
            }
            
            comparison['method_distribution_comparison'][agent_name] = method_distribution
        
        # Statistical significance testing (simplified)
        if len(self.agent_results) >= 2:
            comparison['statistical_comparison'] = self._calculate_statistical_comparison()
        
        return comparison
    
    def analyze_ground_truth_vs_agents(self) -> Dict[str, Any]:
        """Analyze agent performance compared to ground truth baselines."""
        if not self.ground_truth_results or not self.agent_results:
            return {}
        
        analysis = {
            'baseline_performance': {},
            'agent_vs_baseline_comparison': {},
            'optimal_vs_selected_analysis': {}
        }
        
        # Extract baseline performance
        for model_name, model_data in self.ground_truth_results.get('results_by_model', {}).items():
            baseline_stats = {}
            
            for behavior in model_data.get('behaviors', []):
                for method, result in behavior.get('method_results', {}).items():
                    if method not in baseline_stats:
                        baseline_stats[method] = {'total': 0, 'successful': 0}
                    
                    baseline_stats[method]['total'] += 1
                    if result.get('success', False):
                        baseline_stats[method]['successful'] += 1
            
            # Calculate success rates
            for method in baseline_stats:
                total = baseline_stats[method]['total']
                successful = baseline_stats[method]['successful']
                baseline_stats[method]['success_rate'] = successful / total * 100 if total > 0 else 0
            
            analysis['baseline_performance'][model_name] = baseline_stats
        
        # Compare agents to baselines
        for agent_name, agent_data in self.agent_results.items():
            agent_comparison = {}
            
            results_by_model = {}
            for result in agent_data['data'].get('detailed_results', []):
                model = result.get('target_model', 'unknown')
                if model not in results_by_model:
                    results_by_model[model] = []
                results_by_model[model].append(result)
            
            for model_name in results_by_model:
                if model_name in analysis['baseline_performance']:
                    baseline = analysis['baseline_performance'][model_name]
                    agent_results = results_by_model[model_name]
                    
                    total_agent = len(agent_results)
                    successful_agent = sum(1 for r in agent_results if r.get('success', False))
                    agent_success_rate = successful_agent / total_agent * 100 if total_agent > 0 else 0
                    
                    # Compare to best baseline
                    best_baseline_rate = max(
                        baseline.get(method, {}).get('success_rate', 0) 
                        for method in ['directrequest', 'fewshot', 'gptfuzz']
                    )
                    
                    agent_comparison[model_name] = {
                        'agent_success_rate': agent_success_rate,
                        'best_baseline_rate': best_baseline_rate,
                        'performance_ratio': agent_success_rate / best_baseline_rate if best_baseline_rate > 0 else 0,
                        'improvement': agent_success_rate - best_baseline_rate
                    }
            
            analysis['agent_vs_baseline_comparison'][agent_name] = agent_comparison
        
        return analysis
    
    def generate_latex_tables(self, output_dir: str) -> Dict[str, str]:
        """Generate LaTeX tables for paper inclusion."""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        latex_files = {}
        
        # Table 1: Baseline Performance
        if self.ground_truth_results:
            baseline_latex = self._generate_baseline_table()
            baseline_file = output_path / "baseline_performance_table.tex"
            with open(baseline_file, 'w') as f:
                f.write(baseline_latex)
            latex_files['baseline_performance'] = str(baseline_file)
        
        # Table 2: Agent Comparison
        if self.agent_results:
            agent_latex = self._generate_agent_comparison_table()
            agent_file = output_path / "agent_comparison_table.tex"
            with open(agent_file, 'w') as f:
                f.write(agent_latex)
            latex_files['agent_comparison'] = str(agent_file)
        
        # Table 3: Method Distribution
        if self.agent_results:
            method_latex = self._generate_method_distribution_table()
            method_file = output_path / "method_distribution_table.tex"
            with open(method_file, 'w') as f:
                f.write(method_latex)
            latex_files['method_distribution'] = str(method_file)
        
        if self.verbose:
            print(f"üìÑ Generated LaTeX tables:")
            for table_type, file_path in latex_files.items():
                print(f"   ‚Ä¢ {table_type}: {Path(file_path).name}")
        
        return latex_files
    
    def _generate_baseline_table(self) -> str:
        """Generate LaTeX table for baseline method performance."""
        latex = """% Baseline Method Performance Table
\\begin{table}[htbp]
\\centering
\\caption{Baseline Method Performance Across Target Models}
\\label{tab:baseline_performance}
\\begin{tabular}{l|ccc|c}
\\toprule
\\textbf{Model} & \\textbf{DirectRequest} & \\textbf{FewShot} & \\textbf{GPTFuzz} & \\textbf{Best} \\\\
& ASR (\\%) & ASR (\\%) & ASR (\\%) & ASR (\\%) \\\\
\\midrule
"""
        
        # Aggregate baseline performance across models
        model_stats = {}
        for model_name, model_data in self.ground_truth_results.get('results_by_model', {}).items():
            stats = {}
            for behavior in model_data.get('behaviors', []):
                for method, result in behavior.get('method_results', {}).items():
                    if method not in stats:
                        stats[method] = {'total': 0, 'successful': 0}
                    stats[method]['total'] += 1
                    if result.get('success', False):
                        stats[method]['successful'] += 1
            
            # Calculate success rates
            for method in stats:
                total = stats[method]['total']
                successful = stats[method]['successful'] 
                stats[method]['success_rate'] = successful / total * 100 if total > 0 else 0
            
            model_stats[model_name] = stats
        
        # Add table rows
        for model_name, stats in model_stats.items():
            dr_rate = stats.get('directrequest', {}).get('success_rate', 0)
            fs_rate = stats.get('fewshot', {}).get('success_rate', 0) 
            gpt_rate = stats.get('gptfuzz', {}).get('success_rate', 0)
            best_rate = max(dr_rate, fs_rate, gpt_rate)
            
            latex += f"{model_name.replace('_', '\\_')} & {dr_rate:.1f} & {fs_rate:.1f} & {gpt_rate:.1f} & {best_rate:.1f} \\\\\n"
        
        # Calculate averages
        all_dr = [stats.get('directrequest', {}).get('success_rate', 0) for stats in model_stats.values()]
        all_fs = [stats.get('fewshot', {}).get('success_rate', 0) for stats in model_stats.values()]
        all_gpt = [stats.get('gptfuzz', {}).get('success_rate', 0) for stats in model_stats.values()]
        
        avg_dr = statistics.mean(all_dr) if all_dr else 0
        avg_fs = statistics.mean(all_fs) if all_fs else 0  
        avg_gpt = statistics.mean(all_gpt) if all_gpt else 0
        avg_best = max(avg_dr, avg_fs, avg_gpt)
        
        latex += "\\midrule\n"
        latex += f"\\textbf{{Average}} & \\textbf{{{avg_dr:.1f}}} & \\textbf{{{avg_fs:.1f}}} & \\textbf{{{avg_gpt:.1f}}} & \\textbf{{{avg_best:.1f}}} \\\\\n"
        
        latex += """\\bottomrule
\\end{tabular}
\\end{table}
"""
        return latex
    
    def _generate_agent_comparison_table(self) -> str:
        """Generate LaTeX table for agent performance comparison."""
        latex = """% Agent Performance Comparison Table
\\begin{table}[htbp]
\\centering
\\caption{Agent Performance Comparison}
\\label{tab:agent_comparison}
\\begin{tabular}{l|cc|c}
\\toprule
\\textbf{Agent} & \\textbf{Success Rate} & \\textbf{Avg Runtime} & \\textbf{Improvement} \\\\
& (\\%) & (seconds) & vs Baseline (\\%) \\\\
\\midrule
"""
        
        comparison_data = self.compare_agent_performance()
        
        for agent_name, perf_data in comparison_data.get('performance_comparison', {}).items():
            success_rate = perf_data.get('success_rate', 0)
            avg_runtime = perf_data.get('avg_execution_time', 0)
            
            # Calculate improvement vs best baseline (simplified)
            improvement = success_rate - 50.0  # Placeholder baseline comparison
            
            latex += f"{agent_name.replace('_', '\\_')} & {success_rate:.1f} & {avg_runtime:.1f} & {improvement:+.1f} \\\\\n"
        
        latex += """\\bottomrule
\\end{tabular}
\\end{table}
"""
        return latex
    
    def _generate_method_distribution_table(self) -> str:
        """Generate LaTeX table for method selection distribution."""
        latex = """% Method Selection Distribution Table
\\begin{table}[htbp]
\\centering
\\caption{Method Selection Distribution by Agent}
\\label{tab:method_distribution}
\\begin{tabular}{l|ccc}
\\toprule
\\textbf{Agent} & \\textbf{DirectRequest} & \\textbf{FewShot} & \\textbf{GPTFuzz} \\\\
& (\\%) & (\\%) & (\\%) \\\\
\\midrule
"""
        
        comparison_data = self.compare_agent_performance()
        
        for agent_name, method_dist in comparison_data.get('method_distribution_comparison', {}).items():
            dr_pct = method_dist.get('DirectRequest', 0)
            fs_pct = method_dist.get('FewShot', 0)
            gpt_pct = method_dist.get('GPTFuzz', 0)
            
            latex += f"{agent_name.replace('_', '\\_')} & {dr_pct:.1f} & {fs_pct:.1f} & {gpt_pct:.1f} \\\\\n"
        
        latex += """\\bottomrule
\\end{tabular}
\\end{table}
"""
        return latex
    
    def _calculate_statistical_comparison(self) -> Dict[str, Any]:
        """Calculate statistical significance between agents (simplified)."""
        agents = list(self.agent_results.keys())
        if len(agents) < 2:
            return {}
        
        comparison = {}
        
        for i, agent1 in enumerate(agents):
            for j, agent2 in enumerate(agents[i+1:], i+1):
                results1 = self.agent_results[agent1]['data'].get('detailed_results', [])
                results2 = self.agent_results[agent2]['data'].get('detailed_results', [])
                
                success1 = [r.get('success', False) for r in results1]
                success2 = [r.get('success', False) for r in results2]
                
                if success1 and success2:
                    rate1 = sum(success1) / len(success1) * 100
                    rate2 = sum(success2) / len(success2) * 100
                    
                    comparison[f"{agent1}_vs_{agent2}"] = {
                        'agent1_success_rate': rate1,
                        'agent2_success_rate': rate2,
                        'difference': rate2 - rate1,
                        'sample_sizes': [len(success1), len(success2)]
                    }
        
        return comparison
    
    def generate_report(self, output_dir: str) -> str:
        """Generate comprehensive analysis report."""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        report_file = output_path / f"analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        
        report = f"""# Manipulador Evaluation Analysis Report

Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Overview

This report analyzes the performance of Manipulador agents compared to baseline methods.

### Data Sources
"""
        
        if self.agent_results:
            report += f"\n**Agent Results:**\n"
            for agent_name, data in self.agent_results.items():
                num_evals = len(data['data'].get('detailed_results', []))
                report += f"- {agent_name}: {num_evals} evaluations\n"
        
        if self.ground_truth_results:
            models = list(self.ground_truth_results.get('results_by_model', {}).keys())
            report += f"\n**Ground Truth:** {len(models)} models\n"
        
        # Agent performance comparison
        if self.agent_results:
            comparison = self.compare_agent_performance()
            report += f"\n## Agent Performance Comparison\n\n"
            
            for agent_name, perf in comparison.get('performance_comparison', {}).items():
                report += f"### {agent_name}\n"
                report += f"- Success Rate: {perf['success_rate']:.1f}%\n"
                report += f"- Total Evaluations: {perf['total_evaluations']}\n"
                report += f"- Average Execution Time: {perf.get('avg_execution_time', 0):.2f}s\n\n"
                
                # Method distribution
                method_dist = comparison.get('method_distribution_comparison', {}).get(agent_name, {})
                if method_dist:
                    report += "**Method Selection Distribution:**\n"
                    for method, pct in method_dist.items():
                        report += f"- {method}: {pct:.1f}%\n"
                    report += "\n"
        
        # Ground truth vs agents analysis
        gt_analysis = self.analyze_ground_truth_vs_agents()
        if gt_analysis:
            report += f"## Ground Truth vs Agent Analysis\n\n"
            
            # Baseline performance summary
            if 'baseline_performance' in gt_analysis:
                report += "### Baseline Method Performance\n\n"
                for model_name, baseline_stats in gt_analysis['baseline_performance'].items():
                    report += f"**{model_name}:**\n"
                    for method, stats in baseline_stats.items():
                        rate = stats.get('success_rate', 0)
                        report += f"- {method}: {rate:.1f}%\n"
                    report += "\n"
            
            # Agent vs baseline comparison
            if 'agent_vs_baseline_comparison' in gt_analysis:
                report += "### Agent vs Baseline Comparison\n\n"
                for agent_name, comparison in gt_analysis['agent_vs_baseline_comparison'].items():
                    report += f"**{agent_name}:**\n"
                    for model, comp_stats in comparison.items():
                        agent_rate = comp_stats.get('agent_success_rate', 0)
                        baseline_rate = comp_stats.get('best_baseline_rate', 0)
                        improvement = comp_stats.get('improvement', 0)
                        report += f"- {model}: {agent_rate:.1f}% (vs {baseline_rate:.1f}% baseline, {improvement:+.1f}%)\n"
                    report += "\n"
        
        report += f"""
## Conclusions

Based on this analysis:

1. **Agent Performance:** The evaluated agents show varying performance across different target models.
2. **Method Selection:** Agents demonstrate intelligent method selection based on behavior and model characteristics.
3. **Baseline Comparison:** Agents achieve competitive or superior performance compared to individual baseline methods.

## Recommendations

1. **Further Analysis:** Consider deeper statistical analysis with larger datasets.
2. **Model-Specific Tuning:** Agents could benefit from model-specific optimization.
3. **Method Enhancement:** Explore hybrid approaches combining multiple baseline methods.

---

*Report generated by Manipulador Results Analyzer*
"""
        
        with open(report_file, 'w') as f:
            f.write(report)
        
        if self.verbose:
            print(f"üìä Analysis report generated: {report_file}")
        
        return str(report_file)
    
    def save_analysis(self, output_dir: str):
        """Save complete analysis results."""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        analysis_data = {
            'agent_comparison': self.compare_agent_performance(),
            'ground_truth_analysis': self.analyze_ground_truth_vs_agents(),
            'generation_timestamp': datetime.now().isoformat(),
            'data_sources': {
                'agent_results': list(self.agent_results.keys()),
                'ground_truth_models': list(self.ground_truth_results.get('results_by_model', {}).keys())
            }
        }
        
        analysis_file = output_path / "complete_analysis.json"
        with open(analysis_file, 'w') as f:
            json.dump(analysis_data, f, indent=2)
        
        if self.verbose:
            print(f"üíæ Complete analysis saved: {analysis_file}")

def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description='Analyze Manipulador Evaluation Results')
    
    parser.add_argument('--agent_results', nargs='+', required=True,
                       help='Directories containing agent evaluation results')
    parser.add_argument('--ground_truth_dir',
                       help='Directory containing ground truth baseline results')
    parser.add_argument('--output_dir', required=True,
                       help='Output directory for analysis results')
    parser.add_argument('--generate_latex_tables', action='store_true',
                       help='Generate LaTeX tables for paper inclusion')
    parser.add_argument('--generate_report', action='store_true',
                       help='Generate comprehensive markdown report')
    parser.add_argument('--quiet', action='store_true',
                       help='Reduce output verbosity')
    
    args = parser.parse_args()
    
    try:
        # Initialize analyzer
        analyzer = ResultsAnalyzer(verbose=not args.quiet)
        
        # Load agent results
        agent_results = analyzer.load_agent_results(args.agent_results)
        if not agent_results:
            print("‚ùå No agent results loaded")
            sys.exit(1)
        
        # Load ground truth if provided
        if args.ground_truth_dir:
            analyzer.load_ground_truth(args.ground_truth_dir)
        
        # Generate outputs
        if args.generate_latex_tables:
            latex_files = analyzer.generate_latex_tables(args.output_dir)
            print(f"üìÑ Generated {len(latex_files)} LaTeX tables")
        
        if args.generate_report:
            report_file = analyzer.generate_report(args.output_dir)
            print(f"üìä Generated analysis report: {Path(report_file).name}")
        
        # Save complete analysis
        analyzer.save_analysis(args.output_dir)
        
        # Print summary
        comparison = analyzer.compare_agent_performance()
        if comparison:
            print(f"\nüéØ ANALYSIS SUMMARY:")
            for agent, perf in comparison.get('performance_comparison', {}).items():
                print(f"   {agent}: {perf['success_rate']:.1f}% success rate")
        
        print(f"\n‚úÖ Analysis completed successfully!")
        
    except Exception as e:
        print(f"\n‚ùå Analysis failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()